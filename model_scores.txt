Combined scores:
    full sample
        Linear regression:
            Mean Squared Error (MSE): 0.73
            Train MSE 11.65
            testMSE 0.73
            Root Mean Squared Error (RMSE): 0.85
            R2 score: 0.867145054744445
            Cross-validation scores: [0.93617518 0.89875424 0.91402105 0.87963044 0.95842004 0.94999278
            0.88085216 0.91187875 0.83182724 0.84459648]
            Graph analysis:
                Almost defiently over fitting
        Descition tree:
            Train MSE 13.32
            test MSE 0.06
            Mean Squared Error (MSE): 0.05995934959349593
            Root Mean Squared Error (RMSE): 0.2448659829243252
            R2 Score: 0.989047360318754
            Cross-validation scores: [1.         0.76538988 0.26919328 0.97539062 1.         0.88325391
            1.         0.97743056 0.96704225 0.90740008]
            Highly likely to be overfiting the data
        Random Tree:
            MMean Squared Error (MSE): 1.80
            Train MSE 9.46
            testMSE 1.80
            Root Mean Squared Error (RMSE): 1.34
            R2 score: 0.6715455073449411
            Cross-validation scores: [0.74417294 0.70586738 0.62206131 0.69567798 0.67479957 0.74642363
            0.61666804 0.6590718  0.47239641 0.47617511]
            Slightl overfitting
        Due to the random tree being the only data that does not fit this is currently the best model, though more data is defiently needed
    Undersample
        Linear regression:
            Mean Squared Error (MSE): 0.90
            Train MSE 7.26
            testMSE 0.90
            Root Mean Squared Error (RMSE): 0.95
            R2 score: 0.8492369828901045
            Cross-validation scores: [0.79529972 0.89310445 0.89018494 0.8345745  0.94195568 0.850916
            0.80208921 0.88529763 0.90024884 0.85949627]
            Performs slightly better then normal sample, though still over fitting
        Desition tree:
            Train MSE 9.77
            test MSE 0.06
            Mean Squared Error (MSE): 0.05592105263157895
            Root Mean Squared Error (RMSE): 0.23647632573172933
            R2 Score: 0.9906701328711728
            Cross-validation scores: [0.52733935 1.         1.         0.31388889 0.90942029 0.99741546
            0.98243243 1.         1.         0.97188358]
            Probably still overfitting
        Random forest:
            Mean Squared Error (MSE): 2.09
            Train MSE 5.90
            testMSE 2.09
            Root Mean Squared Error (RMSE): 1.44
            R2 score: 0.651844304595755
            Cross-validation scores: [0.56517694 0.62979227 0.5874407  0.51666182 0.81590992 0.65021551
            0.49285357 0.67914429 0.68916211 0.68090417]
            Performing slightly worst


Triple scores:
    Linear regression:
        Mean Squared Error (MSE): 1.30
        Train MSE 7.13
        testMSE 1.30
        Root Mean Squared Error (RMSE): 1.14
        R2 score: 0.6541592466228594
        Cross-validation scores: [-0.53817787  0.69909349  0.58265214 -0.72034079  0.82350452 -0.62626739
        0.631394    0.67569611  0.54709869  0.5952716 ]
    Desiction tree:
        Train MSE 9.03
        test MSE 0.87
        Mean Squared Error (MSE): 0.87478354978355
        Root Mean Squared Error (RMSE): 0.9352986420302074
        R2 Score: 0.7696678880932919
        Cross-validation scores: [-2.03660839  0.10090326  0.73226033 -1.47034688  0.74822274 -0.02169163
        0.65006679  0.6729204   0.31349601  0.65911969]
    Random Tree:
        Mean Squared Error (MSE): 1.27
        Train MSE 7.09
        testMSE 1.27
        Root Mean Squared Error (RMSE): 1.13
        R2 score: 0.6609789871036807
        Cross-validation scores: [-0.05037715  0.74176238  0.67923837  0.01874869  0.79061922  0.00718231
        0.64027193  0.75831872  0.53371571  0.51902234]

For combined classes it seems the desctition tree is by far the best model with low MSE and RMSE scores and a high r2. Though the corss val scores do need to be looked at futher. Ultimately the model needs more testing
It seems he best thing is not the undersampling but the original data with random forest, but there needs to be more samples, as currently not enough. x