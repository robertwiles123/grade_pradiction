Combined scores:
    full sample
        Linear regression:
            Mean Squared Error (MSE): 0.45
            Train MSE 11.65
            testMSE 0.45
            Root Mean Squared Error (RMSE): 0.67
            R2 score: 0.9175767454496061
            Cross-validation scores: [0.96361803 1.         0.89202263 0.85812158 0.93786991 0.99981362
            1.         0.90895513 0.96555859 0.88985964]
        Descition tree:
            Train MSE 13.32
            test MSE 0.06
            Mean Squared Error (MSE): 0.05995934959349593
            Root Mean Squared Error (RMSE): 0.2448659829243252
            R2 Score: 0.989047360318754
            Cross-validation scores: [1.         0.76538988 0.26919328 0.97539062 1.         0.88325391
            1.         0.97743056 0.96704225 0.90740008]
            Highly likely to be overfiting the data
        Random Tree:
            Mean Squared Error (MSE): 1.25
            Train MSE 9.97
            testMSE 1.25
            Root Mean Squared Error (RMSE): 1.12
            R2 score: 0.7716653083401249
            Cross-validation scores: [0.89543001 0.8202554  0.74076106 0.75522936 0.77915674 0.85202925
            0.71221011 0.81441212 0.70439655 0.6040331 ]
            Slightl overfitting
        Ridge:
            Mean Squared Error (MSE): 0.44
            Train MSE 11.19
            testMSE 0.44
            Root Mean Squared Error (RMSE): 0.67
            R2 score: 0.9058417472525796
            Cross-validation scores: [0.95429777 0.92138909 0.93239547 0.87787831 0.96700335 0.95198109
            0.95312472 0.92230103 0.87987016 0.86261458]
            It seems there is a slight over fitting to the data, though it does seem to be the best at the moment
        Out of these the Ridge seems to be performing the best. Due to the lowest MSE and a relatively low RMSE. It also has a predictive power of 90.58%. The cross value scores are also extremely close. It is likely slightly over fitting, though this will imporve with data.
    Undersample
        Linear regression:
            Mean Squared Error (MSE): 0.70
            Train MSE 7.68
            testMSE 0.70
            Root Mean Squared Error (RMSE): 0.84
            R2 score: 0.8825534373194684
            Cross-validation scores: [0.89217792 0.94117223 0.95824329 0.90257774 0.9668687  0.87972761
            0.88619514 0.95228749 0.94822232 0.90780183]
            Performs slightly better then normal sample, though still over fitting
        Desition tree:
            Train MSE 9.62
            test MSE 0.06
            Mean Squared Error (MSE): 0.05921052631578947
            Root Mean Squared Error (RMSE): 0.2433321316961438
            R2 Score: 0.9901213171577123
            Cross-validation scores: [0.10963923 1.         1.         0.99476651 0.90942029 0.99741546
            0.99932432 1.         1.         0.89695222]
            Probably still overfitting
        Random forest:
            Mean Squared Error (MSE): 1.66
            Train MSE 6.52
            testMSE 1.66
            Root Mean Squared Error (RMSE): 1.29
            R2 score: 0.7222992489890236
            Cross-validation scores: [0.66915965 0.7087335  0.73878312 0.56545941 0.89243459 0.77650406
            0.63370123 0.80101886 0.76730621 0.75112647]
            Performing slightly worst
        Ridge:
            Mean Squared Error (MSE): 0.38
            Train MSE 9.19
            testMSE 0.38
            Root Mean Squared Error (RMSE): 0.62
            R2 score: 0.9384619960279665
            Cross-validation scores: [0.85808936 0.90252833 0.94095465 0.86792006 0.94840457 0.86679435
            0.84809774 0.93025941 0.92734545 0.87093712]
        Again the ridge model seems to be doing the best. Although the desition tree seems to have a better score. The learning curve seems to suggest that overfiting is prevalent.
    The under fitted ridge model seems to be performing the best with the current data. Both MSE and RMSE scores are lower with the R2 value being around 3% higher. There is also more consistency with the cross_val_score.
    The underfitted data is actualyl perfoming better for all models except random forest.
Triple scores:
    Linear regression:
        Mean Squared Error (MSE): 12.79
        Train MSE 15.09
        testMSE 12.79
        Root Mean Squared Error (RMSE): 3.58
        R2 score: -2.515586194016398
        Cross-validation scores: [-8.83658744e+00 -6.59105896e+00 -7.02279759e+25 -2.56094078e+02
        -3.61823943e+01 -1.36722196e+00 -7.53026467e-01 -1.53812461e+00
        -5.83462539e-01 -1.73537584e+26]
    Desiction tree:
        Train MSE 9.01
        test MSE 0.89
        Mean Squared Error (MSE): 0.8863636363636364
        Root Mean Squared Error (RMSE): 0.9414688716912718
        R2 Score: 0.7666014476436341
        Cross-validation scores: [-2.03660839  0.10090326  0.73226033 -1.47034688  0.74822274 -0.02169163
        0.65006679  0.62011836  0.31349601  0.65911969]
    Random Tree:
        Mean Squared Error (MSE): 1.17
        Train MSE 7.27
        testMSE 1.17
        Root Mean Squared Error (RMSE): 1.08
        R2 score: 0.6892667130026106
        Cross-validation scores: [-0.52870285  0.65738317  0.64536833  0.1368927   0.77138643 -0.06681998
        0.69876951  0.77084128  0.56903928  0.57490778]
    Ridge:
        Mean Squared Error (MSE): 1.62
        Train MSE 10.09
        testMSE 1.62
        Root Mean Squared Error (RMSE): 1.27
        R2 score: 0.6057414800732545
        Cross-validation scores: [-0.59554786  0.6513534   0.56871245 -0.83030488  0.81423953 -0.62501275
        0.62107563  0.66777364  0.513296    0.61409629]
    It looks like the desiction tree is performing best out of these models. however there is a major issue with all of them, and that is none are fitting the data well.

I have been treating the data as contius as I transormed it. However, it is porbably best to go back and treat it as the ordinal it actually is.

For combined classes it seems the desctition tree is by far the best model with low MSE and RMSE scores and a high r2. Though the corss val scores do need to be looked at futher. Ultimately the model needs more testing
It seems he best thing is not the undersampling but the original data with random forest, but there needs to be more samples, as currently not enough. x